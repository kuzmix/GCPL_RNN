{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuzmix/GCPL_RNN/blob/Colab-training/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9v8aIYl2dHq"
      },
      "outputs": [],
      "source": [
        "\"\"\"For google colab workflow - mounts Google Drive and go to it\"\"\"\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('./gdrive/MyDrive/Projects/GCPL_RNN')"
      ],
      "id": "_9v8aIYl2dHq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60d828c1"
      },
      "outputs": [],
      "source": [
        "\"\"\"For local machine - go to root directory\"\"\"\n",
        "import os\n",
        "os.chdir('../')"
      ],
      "id": "60d828c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7d89eca-faca-4d2a-8308-5d23e961cb86"
      },
      "outputs": [],
      "source": [
        "\"\"\"Imports all necessary libs\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from itertools import product\n",
        "import pickle\n",
        "import math\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import scipy as sp\n",
        "from scipy.optimize import curve_fit\n",
        "import sklearn\n",
        "from Code.setup import *\n",
        "import datetime as dt\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import RNN_for_GCPL.setup"
      ],
      "id": "d7d89eca-faca-4d2a-8308-5d23e961cb86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae86c06f"
      },
      "outputs": [],
      "source": [
        "\"\"\"For compatibility - cd to folder with data and models\"\"\"\n",
        "os.chdir('../RNN_for_GCPL/')"
      ],
      "id": "ae86c06f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123c730c"
      },
      "outputs": [],
      "source": [
        "savepath = r'data/v4/30/main'\n",
        "dataset_validate = GCPL_dataset_resampled3(r'data/v4/120/validate')\n",
        "dataset_rs2 = GCPL_dataset_resampled3(savepath)"
      ],
      "id": "123c730c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b8df914"
      },
      "outputs": [],
      "source": [
        "seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
        "input_dim = 2\n",
        "output_dim = 20\n",
        "num_layers = 2\n",
        "bidir= True\n",
        "lr = 5e-4\n",
        "init_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
        "criterion = nn.MSELoss(reduction='none')\n",
        "best_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
        "# kf = KFold(5, shuffle=True, random_state=DEFAULT_RANDOM_SEED)\n",
        "gkf = GroupKFold(4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "7b8df914"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42358b3e"
      },
      "outputs": [],
      "source": [],
      "id": "42358b3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpCEkaxLX44A"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "# params = [ (30,3,False), (60, 4, True)]\n",
        "params = [(20, 2, False), (20, 2, True), (30, 3, True)]\n",
        "for a,b,c in params:\n",
        "    model = MyGRU(input_dim, a, num_layers=b, bidir=c)\n",
        "    models.append(model)\n",
        "sampling = [30, 60, 120, 180, 300]"
      ],
      "id": "QpCEkaxLX44A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a7AUIWKTCdT"
      },
      "outputs": [],
      "source": [
        "probs = balancing(dataset_rs2, 10)\n",
        "soh, info = statistics(dataset_rs2)\n",
        "clear_output()"
      ],
      "id": "_a7AUIWKTCdT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73db179b"
      },
      "outputs": [],
      "source": [
        "class BucketSampler(torch.utils.data.Sampler):\n",
        "    \n",
        "    def __init__(self, lengths, buckets=(50,500,50), shuffle=True, batch_size=32, drop_last=False):\n",
        "        \n",
        "        super().__init__(lengths)\n",
        "        \n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "        \n",
        "        assert isinstance(buckets, tuple)\n",
        "        bmin, bmax, bstep = buckets\n",
        "        assert (bmax - bmin) % bstep == 0\n",
        "        \n",
        "        buckets = defaultdict(list)\n",
        "        for i, length in enumerate(lengths):\n",
        "            if length > bmin:\n",
        "                bucket_size = min((length // bstep) * bstep, bmax)\n",
        "                buckets[bucket_size].append(i)\n",
        "                \n",
        "        self.buckets = dict()\n",
        "        for bucket_size, bucket in buckets.items():\n",
        "            if len(bucket) > 0:\n",
        "                self.buckets[bucket_size] = torch.tensor(bucket, dtype=torch.int, device='cpu')\n",
        "        \n",
        "        # call __iter__() to store self.length\n",
        "        self.__iter__()\n",
        "            \n",
        "    def __iter__(self):\n",
        "        \n",
        "        if self.shuffle == True:\n",
        "            for bucket_size in self.buckets.keys():\n",
        "                self.buckets[bucket_size] = self.buckets[bucket_size][torch.randperm(self.buckets[bucket_size].nelement())]\n",
        "                \n",
        "        batches = []\n",
        "        for bucket in self.buckets.values():\n",
        "            curr_bucket = torch.split(bucket, self.batch_size)\n",
        "            if len(curr_bucket) > 1 and self.drop_last == True:\n",
        "                if len(curr_bucket[-1]) < len(curr_bucket[-2]):\n",
        "                    curr_bucket = curr_bucket[:-1]\n",
        "            batches += curr_bucket\n",
        "            \n",
        "        self.length = len(batches)\n",
        "        \n",
        "        if self.shuffle == True:\n",
        "            random.shuffle(batches)\n",
        "            \n",
        "        return iter(batches)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n"
      ],
      "id": "73db179b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90a9d62a"
      },
      "outputs": [],
      "source": [
        "def collate_batch_length(batch):\n",
        "    sample_list = []\n",
        "    label_list = []\n",
        "    lengths = []\n",
        "    for i in batch:\n",
        "        sample = np.stack([i['E'], i['I']],axis=-1)\n",
        "        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n",
        "        label_list.append(i['SoH'])\n",
        "        lengths.append(len(i['E']))\n",
        "    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n",
        "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
        "    lengths_torch = torch.tensor(lengths, dtype=torch.float32)\n",
        "    return sequence_pad, labels, lengths_torch  \n"
      ],
      "id": "90a9d62a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e6c4970"
      },
      "outputs": [],
      "source": [
        "missed_length = []\n",
        "n_bins = 10\n",
        "_ , ind = statistics(dataset_rs2)\n",
        "for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_rs2, groups=info.Pouch)):\n",
        "    seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
        "    train_set = torch.utils.data.Subset(dataset_rs2, train_indices)\n",
        "    \n",
        "    min_length =  ind.loc[train_indices,'Len'].min()\n",
        "    max_length = ind.loc[train_indices,'Len'].max()\n",
        "    max_length += n_bins - (max_length-min_length)% n_bins\n",
        "    print(min_length, max_length)\n",
        "    val_set = torch.utils.data.Subset(dataset_rs2, val_indices)\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n",
        "    bucket_sampler = BucketSampler(ind.loc[train_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
        "    # train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler, collate_fn=collate_batch_length)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size =batch_size, shuffle=True, collate_fn=collate_batch_length)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    miss_ = []\n",
        "    for data, labels, length in train_loader:\n",
        "        m = data.shape[0]- length.mean()\n",
        "        miss_.append([m, data.shape[0],length.mean()])\n",
        "    miss_ = torch.tensor(miss_)\n",
        "    missed_length.append(miss_)\n",
        "    print(miss_.mean(axis=0))\n"
      ],
      "id": "7e6c4970"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a6bb4c33"
      },
      "outputs": [],
      "source": [
        "draw_every = 20\n",
        "num_epochs = 100\n",
        "save_every = 10\n",
        "batch_size = 64\n",
        "n_bins = 10\n",
        "drop_after = 20\n",
        "model_version = 14\n",
        "for m in sampling[3:]:\n",
        "    dataset_rs2 = GCPL_dataset_resampled3(f'data/v4/{m}/main')\n",
        "    dataset_validate = GCPL_dataset_resampled3(f'data/v4/{m}/validate')\n",
        "    for l, init_model in enumerate(models):\n",
        "        path = f'./models/v{model_version}/{m}/{l}'\n",
        "        for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_rs2, groups=info.Pouch)):\n",
        "            seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
        "            min_length =  info.loc[train_indices,'Len'].min()\n",
        "            max_length = info.loc[train_indices,'Len'].max()\n",
        "            max_length += n_bins - (max_length-min_length)% n_bins\n",
        "            prob_train = probs.copy()\n",
        "            prob_train[val_indices] = 0\n",
        "            \n",
        "            val_set = torch.utils.data.Subset(dataset_rs2, val_indices)\n",
        "            # sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n",
        "            # train_loader = torch.utils.data.DataLoader(train_set, batch_size, collate_fn=collate_batch, sampler=sampler)\n",
        "            # val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "            \n",
        "            bucket_sampler_val = BucketSampler(info.loc[val_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
        "            \n",
        "            val_loader = torch.utils.data.DataLoader(val_set, batch_size =1,  batch_sampler=bucket_sampler_val, collate_fn=collate_batch)\n",
        "            model = copy.deepcopy(init_model)\n",
        "            model.to(device)\n",
        "            optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            sheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=6)\n",
        "            handler = ModelHandler(model, 1e6, f'20 neurons, 2 layers, 120s sampling, 80% train, 20% val, k-fold No {k}.', path= path, kfold=True)\n",
        "            pp = ProgressPlotter()\n",
        "            counter = 0\n",
        "            for i in range(num_epochs):\n",
        "                balance = torch.multinomial(torch.tensor(prob_train) , len(train_indices), replacement=True)\n",
        "                train_set = torch.utils.data.Subset(dataset_rs2, balance)\n",
        "                bucket_sampler_train = BucketSampler(info.loc[balance.numpy(),'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
        "                train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler_train, collate_fn=collate_batch)\n",
        "\n",
        "                model.train()\n",
        "                loss_val = 0\n",
        "                loss_train = 0\n",
        "                for data, labels in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    preds = model(data.to(device))\n",
        "                    loss = torch.sum(criterion(preds, labels.to(device)))\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    loss_train += loss.detach().cpu()\n",
        "                \n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    loss_train = loss_train/len(train_set)\n",
        "                    pp.add_scalar('loss_train', loss_train.detach().cpu().numpy())\n",
        "                    model.eval()\n",
        "                    for data, labels in val_loader:\n",
        "                        preds = model(data.to(device))\n",
        "                        loss = torch.sum(criterion(preds, labels.to(device)))\n",
        "                        loss_val += loss.detach().cpu()\n",
        "                    loss_val = loss_val/len(val_set)\n",
        "                    sheduler.step(loss_val)\n",
        "                    handler.check_loss(loss_val, draw_every)\n",
        "                    pp.add_scalar('loss_val', loss_val.detach().cpu().numpy())\n",
        "                    if loss_val > handler.best_loss:\n",
        "                        counter +=1\n",
        "                    else:\n",
        "                        counter = 0\n",
        "                    if (i+1)%draw_every == 0:\n",
        "                        pp.display([['loss_train', 'loss_val']])\n",
        "                    print(i, counter, loss_train*2500, loss_val*2500)\n",
        "                    if counter > drop_after:\n",
        "                        break\n",
        "                if (i+1)%save_every==0:\n",
        "                    handler.save(kfold_number=k)\n",
        "                    \n",
        "                    \n",
        "            handler.add_pp(pp)\n",
        "            handler.display()\n",
        "            handler.save(kfold_number=k)\n",
        "            with torch.no_grad():\n",
        "                loss_val = []\n",
        "                for data, labels in val_loader:\n",
        "                    preds = model(data.to(device))\n",
        "                    loss = criterion(preds, labels.to(device))\n",
        "                    loss_val.append(loss)\n",
        "                loss_val = torch.hstack(loss_val)\n",
        "                print(loss_val.mean(), loss_val.std())\n"
      ],
      "id": "a6bb4c33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh_mcWwvFvr8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Loss checking\"\"\"\n",
        "loss = []\n",
        "model = copy.deepcopy(init_model) \n",
        "path= f'./models/v127/2'\n",
        "handler = ModelHandler(model, 1e6, path= f'./models/v2/', kfold=True)\n",
        "batches = next(os.walk(path))[1]\n",
        "loss_ = []\n",
        "epochs_ = []\n",
        "for batch in batches:\n",
        "    batchpath = path+ '/' + batch\n",
        "    handler.load(batchpath)\n",
        "    loss_.append(handler.best_loss.cpu())\n",
        "loss.append(loss_)\n",
        "loss = np.array(loss)*2500\n",
        "print(str(np.mean(loss))+ '±' + str(np.std(loss)))\n",
        "loss"
      ],
      "id": "Xh_mcWwvFvr8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu4jzm6aODXM"
      },
      "outputs": [],
      "source": [
        "def load(path_):\n",
        "    if path_.split('.')[-1] == 'pt':\n",
        "        path = '/'.join(path.split('/')[:-1])\n",
        "        name = path_.split('/')[-1]\n",
        "        checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n",
        "        loss = [checkpoint['loss'].numpy()]\n",
        "    else:\n",
        "        path = path_\n",
        "        files = next(os.walk(path_))[2]\n",
        "        x = lambda x: int(x.split('_')[-1].split('.')[0].split('(')[0])\n",
        "        files.sort(key=x)\n",
        "        loss = []\n",
        "        for name in files:\n",
        "            checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n",
        "            loss.append(checkpoint['loss'].numpy())\n",
        "    return np.array(loss)\n"
      ],
      "id": "Vu4jzm6aODXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ae63b29"
      },
      "outputs": [],
      "source": [
        "path = f'./models/v10'\n",
        "loss = {}\n",
        "for root, dirs, files in os.walk(path):\n",
        "    if dirs:\n",
        "        continue\n",
        "    if not files:\n",
        "        continue\n",
        "\n",
        "    loss[root] = load(root)*2500\n",
        "loss"
      ],
      "id": "7ae63b29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd95124"
      },
      "outputs": [],
      "source": [
        "def collate_batch_named(batch, named=True):\n",
        "    sample_list = []\n",
        "    label_list = []\n",
        "    pouch = []\n",
        "    cycle = []\n",
        "    for i in batch:\n",
        "        sample = np.stack([i['E'], i['I']],axis=-1)\n",
        "        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n",
        "        label_list.append(i['SoH'])\n",
        "        pouch.append(i['Pouch'])\n",
        "        cycle.append(i['Cycle'])\n",
        "    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n",
        "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
        "    if not named:\n",
        "        return sequence_pad, labels    \n",
        "    else:\n",
        "        return sequence_pad, labels, pouch, cycle"
      ],
      "id": "afd95124"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c43337c"
      },
      "outputs": [],
      "source": [
        "def detailed_loss(handler, val_loader):\n",
        "    with torch.no_grad():\n",
        "        loss = []\n",
        "        pouches = []\n",
        "        soh = []\n",
        "        pred = []\n",
        "        cycles = []\n",
        "        for data, labels, pouch, cycle in val_loader:\n",
        "            preds = handler.best_model(data)\n",
        "            pred.extend(preds)\n",
        "            loss_ = criterion(preds, labels)\n",
        "            loss.append(loss_)\n",
        "            pouches.extend(pouch)\n",
        "            cycles.extend(cycle)\n",
        "            soh.extend(labels)\n",
        "        \n",
        "        loss = torch.hstack(loss)\n",
        "        soh = torch.Tensor(soh)*50+50\n",
        "        pred = torch.Tensor(pred)*50+50\n",
        "        loss *= 2500 \n",
        "        results = pd.DataFrame({'Pouch':pouches,'Cycle':cycles, 'Loss':loss, 'SoH':soh,'Pred':pred} )\n",
        "        # results.sort_values(by='Loss',axis=0, ascending=0,inplace=True)\n",
        "        return results"
      ],
      "id": "5c43337c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "593e1393"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "model_version = 10\n",
        "losses = []\n",
        "for k in range(3):\n",
        "    for s in sampling:\n",
        "        datapath = f'data/v4/{s}/'\n",
        "        modelpath = f'./models/v{model_version}/{s}/{k}'\n",
        "        dataset_validate = GCPL_dataset_resampled3(datapath+'validate')\n",
        "        dataset_rs2 = GCPL_dataset_resampled3(datapath+'main')\n",
        "        main_loader = torch.utils.data.DataLoader(dataset_rs2, batch_size, shuffle=False, collate_fn=collate_batch_named)\n",
        "        val_loader = torch.utils.data.DataLoader(dataset_validate, batch_size, shuffle=False, collate_fn=collate_batch_named)\n",
        "        for i in next(os.walk(modelpath))[1]:\n",
        "            init_model = copy.deepcopy(models[k])\n",
        "            path = modelpath + f'/{i}'\n",
        "            print(path)\n",
        "            handler = ModelHandler(init_model)\n",
        "            handler.load(path)\n",
        "            result_main = detailed_loss(handler, main_loader)\n",
        "            result_val = detailed_loss(handler, val_loader)\n",
        "            result = pd.concat([result_main, result_val])\n",
        "            loss = pd.concat([result.groupby('Pouch').Loss.mean(), result.groupby('Pouch').Loss.std()], axis=1)\n",
        "            loss.columns = [f'{s}/{k}/{i}_Mean', f'{s}/{k}/{i}_Std']\n",
        "            losses.append(loss)\n",
        "f_loss = pd.concat(losses, axis=1)\n"
      ],
      "id": "593e1393"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk6GHM8aauNh"
      },
      "outputs": [],
      "source": [
        "loss = {}\n",
        "for s in sampling:\n",
        "    l_ = {}\n",
        "    for i in range(3):\n",
        "        path = f'./models/v10/{s}/{i}'\n",
        "        batches = next(os.walk(path))[1]\n",
        "        loss_ = []\n",
        "        for batch in batches:\n",
        "            batchpath = path+ '/' + batch\n",
        "            a = load(batchpath)\n",
        "            loss_.append(a)\n",
        "        loss_ = torch.tensor(loss_)*2500\n",
        "        l_[i] = loss_\n",
        "        print(s, i, f'{loss_.mean():.3}±{loss_.std():.3}')\n",
        "    loss[s] = l_"
      ],
      "id": "pk6GHM8aauNh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykpy2gbcdkg"
      },
      "outputs": [],
      "source": [
        "length = []\n",
        "soh = []\n",
        "info = pd.DataFrame({\n",
        "            'Pouch':[],\n",
        "            'SoH':[],\n",
        "            \"Len\":[],\n",
        "            })\n",
        "for i in range(len(dataset_rs2)):\n",
        "    one = len(dataset_rs2[i]['E'])\n",
        "    soh = dataset_rs2[i]['SoH']*50+50\n",
        "    if np.any(np.isnan(dataset_rs2[i]['E'])):\n",
        "        print(i)\n",
        "    pouch = dataset_rs2[i]['Pouch']\n",
        "    if i%1000 == 0:\n",
        "        print(i)\n",
        "    info.loc[len(info)] = [pouch, soh, one]\n",
        "info"
      ],
      "id": "3ykpy2gbcdkg"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}