{"cells":[{"cell_type":"code","execution_count":null,"id":"_9v8aIYl2dHq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16370,"status":"ok","timestamp":1683981592458,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"_9v8aIYl2dHq","outputId":"ef3e7b33-10c8-40cb-893f-b740d1012a86"},"outputs":[],"source":["\"\"\"For google colab workflow - mounts Google Drive and go to it\"\"\"\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","os.chdir('./gdrive/MyDrive/Projects/RNN_for_GCPL')"]},{"cell_type":"code","execution_count":null,"id":"60d828c1","metadata":{},"outputs":[],"source":["\"\"\"For local machine - go to root directory\"\"\"\n","import os\n","os.chdir('../')"]},{"cell_type":"code","execution_count":null,"id":"d7d89eca-faca-4d2a-8308-5d23e961cb86","metadata":{"executionInfo":{"elapsed":8925,"status":"ok","timestamp":1683981601379,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"d7d89eca-faca-4d2a-8308-5d23e961cb86"},"outputs":[],"source":["\"\"\"Imports all necessary libs\"\"\"\n","import os\n","import sys\n","import numpy as np\n","from sys import getsizeof\n","from tqdm import tqdm\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import itertools\n","from itertools import product\n","import pickle\n","import math\n","import shutil\n","import torch\n","from torch.utils.data import Dataset\n","import scipy as sp\n","from scipy.optimize import curve_fit\n","import sklearn\n","from Code.setup import *\n","import datetime as dt\n","import time\n","import torch.nn as nn\n","import copy\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GroupKFold\n","import RNN_for_GCPL.setup"]},{"cell_type":"code","execution_count":null,"id":"91a67b22","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ae86c06f","metadata":{},"outputs":[],"source":["\"\"\"For compatibility - cd to folder with data and models\"\"\"\n","os.chdir('../RNN_for_GCPL/')"]},{"cell_type":"code","execution_count":null,"id":"123c730c","metadata":{"executionInfo":{"elapsed":4985,"status":"ok","timestamp":1683981757641,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"123c730c"},"outputs":[],"source":["savepath = r'data/v4/30/main'\n","dataset_validate = GCPL_dataset_resampled3(r'data/v4/120/validate')\n","dataset_rs2 = GCPL_dataset_resampled3(savepath)"]},{"cell_type":"code","execution_count":null,"id":"26cb02ba","metadata":{},"outputs":[],"source":["dataset_rs2[2]"]},{"cell_type":"code","execution_count":null,"id":"7b8df914","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683983697695,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"7b8df914"},"outputs":[],"source":["seedEverything(seed=DEFAULT_RANDOM_SEED)\n","input_dim = 2\n","output_dim = 20\n","num_layers = 2\n","bidir= True\n","lr = 5e-4\n","init_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n","criterion = nn.MSELoss(reduction='none')\n","best_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n","# kf = KFold(5, shuffle=True, random_state=DEFAULT_RANDOM_SEED)\n","gkf = GroupKFold(4)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"42358b3e","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"QpCEkaxLX44A","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1683981655651,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"QpCEkaxLX44A"},"outputs":[],"source":["models = []\n","# params = [ (30,3,False), (60, 4, True)]\n","params = [(20, 2, False), (20, 2, True), (30, 3, True)]\n","for a,b,c in params:\n","    model = MyGRU(input_dim, a, num_layers=b, bidir=c)\n","    models.append(model)\n","sampling = [30, 60, 120, 180, 300]"]},{"cell_type":"code","execution_count":null,"id":"_a7AUIWKTCdT","metadata":{"id":"_a7AUIWKTCdT"},"outputs":[],"source":["probs = balancing(dataset_rs2, 10)\n","soh, info = statistics(dataset_rs2)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"id":"73db179b","metadata":{},"outputs":[],"source":["class BucketSampler(torch.utils.data.Sampler):\n","    \n","    def __init__(self, lengths, buckets=(50,500,50), shuffle=True, batch_size=32, drop_last=False):\n","        \n","        super().__init__(lengths)\n","        \n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.drop_last = drop_last\n","        \n","        assert isinstance(buckets, tuple)\n","        bmin, bmax, bstep = buckets\n","        assert (bmax - bmin) % bstep == 0\n","        \n","        buckets = defaultdict(list)\n","        for i, length in enumerate(lengths):\n","            if length > bmin:\n","                bucket_size = min((length // bstep) * bstep, bmax)\n","                buckets[bucket_size].append(i)\n","                \n","        self.buckets = dict()\n","        for bucket_size, bucket in buckets.items():\n","            if len(bucket) > 0:\n","                self.buckets[bucket_size] = torch.tensor(bucket, dtype=torch.int, device='cpu')\n","        \n","        # call __iter__() to store self.length\n","        self.__iter__()\n","            \n","    def __iter__(self):\n","        \n","        if self.shuffle == True:\n","            for bucket_size in self.buckets.keys():\n","                self.buckets[bucket_size] = self.buckets[bucket_size][torch.randperm(self.buckets[bucket_size].nelement())]\n","                \n","        batches = []\n","        for bucket in self.buckets.values():\n","            curr_bucket = torch.split(bucket, self.batch_size)\n","            if len(curr_bucket) > 1 and self.drop_last == True:\n","                if len(curr_bucket[-1]) < len(curr_bucket[-2]):\n","                    curr_bucket = curr_bucket[:-1]\n","            batches += curr_bucket\n","            \n","        self.length = len(batches)\n","        \n","        if self.shuffle == True:\n","            random.shuffle(batches)\n","            \n","        return iter(batches)\n","    \n","    def __len__(self):\n","        return self.length\n"]},{"cell_type":"code","execution_count":null,"id":"90a9d62a","metadata":{},"outputs":[],"source":["def collate_batch_length(batch):\n","    sample_list = []\n","    label_list = []\n","    lengths = []\n","    for i in batch:\n","        sample = np.stack([i['E'], i['I']],axis=-1)\n","        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n","        label_list.append(i['SoH'])\n","        lengths.append(len(i['E']))\n","    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n","    labels = torch.tensor(label_list, dtype=torch.float32)\n","    lengths_torch = torch.tensor(lengths, dtype=torch.float32)\n","    return sequence_pad, labels, lengths_torch  \n"]},{"cell_type":"code","execution_count":null,"id":"7e6c4970","metadata":{},"outputs":[],"source":["missed_length = []\n","n_bins = 10\n","_ , ind = statistics(dataset_rs2)\n","for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_rs2, groups=info.Pouch)):\n","    seedEverything(seed=DEFAULT_RANDOM_SEED)\n","    train_set = torch.utils.data.Subset(dataset_rs2, train_indices)\n","    \n","    min_length =  ind.loc[train_indices,'Len'].min()\n","    max_length = ind.loc[train_indices,'Len'].max()\n","    max_length += n_bins - (max_length-min_length)% n_bins\n","    print(min_length, max_length)\n","    val_set = torch.utils.data.Subset(dataset_rs2, val_indices)\n","    sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n","    bucket_sampler = BucketSampler(ind.loc[train_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n","    # train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler, collate_fn=collate_batch_length)\n","    train_loader = torch.utils.data.DataLoader(train_set, batch_size =batch_size, shuffle=True, collate_fn=collate_batch_length)\n","\n","    val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n","    miss_ = []\n","    for data, labels, length in train_loader:\n","        m = data.shape[0]- length.mean()\n","        miss_.append([m, data.shape[0],length.mean()])\n","    miss_ = torch.tensor(miss_)\n","    missed_length.append(miss_)\n","    print(miss_.mean(axis=0))\n"]},{"cell_type":"code","execution_count":null,"id":"a6bb4c33","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":325},"executionInfo":{"elapsed":1189374,"status":"ok","timestamp":1683993085823,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"a6bb4c33","outputId":"eb71b146-75b8-40cd-ab1e-5e35dca8d0b9"},"outputs":[],"source":["draw_every = 25\n","num_epochs = 75\n","save_every = 10\n","batch_size = 64\n","n_bins = 10\n","_ , ind = statistics(dataset_rs2)\n","for m in sampling:\n","    dataset_rs2 = GCPL_dataset_resampled3(f'data/v4/{m}/main')\n","    dataset_validate = GCPL_dataset_resampled3(f'data/v4/{m}/validate')\n","    for l, init_model in enumerate(models):\n","        path = f'./models/v10/{m}/{l}'\n","        for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_rs2, groups=info.Pouch)):\n","            seedEverything(seed=DEFAULT_RANDOM_SEED)\n","            min_length =  ind.loc[train_indices,'Len'].min()\n","            max_length = ind.loc[train_indices,'Len'].max()\n","            max_length += n_bins - (max_length-min_length)% n_bins\n","            train_set = torch.utils.data.Subset(dataset_rs2, train_indices)\n","            val_set = torch.utils.data.Subset(dataset_rs2, val_indices)\n","            # sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n","            # train_loader = torch.utils.data.DataLoader(train_set, batch_size, collate_fn=collate_batch, sampler=sampler)\n","            # val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n","            bucket_sampler_train = BucketSampler(ind.loc[train_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n","            bucket_sampler_val = BucketSampler(ind.loc[val_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n","            train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler_train, collate_fn=collate_batch)\n","            model = copy.deepcopy(init_model)\n","            val_loader = torch.utils.data.DataLoader(val_set, batch_size =1,  batch_sampler=bucket_sampler_val, collate_fn=collate_batch)\n","            model = copy.deepcopy(init_model)\n","            model.to(device)\n","            optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n","            sheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n","            handler = ModelHandler(model, 1e6, f'20 neurons, 2 layers, 120s sampling, 80% train, 20% val, k-fold No {k}.', path= path, kfold=True)\n","            pp = ProgressPlotter()\n","            counter = 0\n","            for i in range(num_epochs):\n","                model.train()\n","                loss_val = 0\n","                loss_train = 0\n","                for data, labels in train_loader:\n","                    optimizer.zero_grad()\n","                    preds = model(data.to(device))\n","                    loss = torch.sum(criterion(preds, labels.to(device)))\n","                    loss.backward()\n","                    optimizer.step()\n","                    loss_train += loss.detach().cpu()\n","                \n","                \n","                with torch.no_grad():\n","                    loss_train = loss_train/len(train_loader)\n","                    pp.add_scalar('loss_train', loss_train.detach().cpu().numpy())\n","                    model.eval()\n","                    for data, labels in val_loader:\n","                        preds = model(data.to(device))\n","                        loss = torch.sum(criterion(preds, labels.to(device)))\n","                        loss_val += loss.detach().cpu()\n","                    loss_val = loss_val/len(val_set)\n","                    sheduler.step(loss_val)\n","                    handler.check_loss(loss_val, draw_every)\n","                    pp.add_scalar('loss_val', loss_val.detach().cpu().numpy())\n","                    if loss_val > handler.best_loss:\n","                        counter +=1\n","                    else:\n","                        counter = 0\n","                    if (i+1)%draw_every == 0:\n","                        pp.display([['loss_train', 'loss_val']])\n","                    print(i, counter, loss_train*2500, loss_val*2500)\n","                if (i+1)%save_every==0:\n","                    handler.save(kfold_number=k)\n","                    \n","                    \n","            handler.add_pp(pp)\n","            handler.display()\n","            handler.save(kfold_number=k)\n","            with torch.no_grad():\n","                loss_val = []\n","                for data, labels in val_loader:\n","                    preds = model(data.to(device))\n","                    loss = criterion(preds, labels.to(device))\n","                    loss_val.append(loss)\n","                loss_val = torch.hstack(loss_val)\n","                print(loss_val.mean(), loss_val.std())\n"]},{"cell_type":"code","execution_count":null,"id":"cd5758cd","metadata":{},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"id":"Xh_mcWwvFvr8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1682582736290,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"Xh_mcWwvFvr8","outputId":"d50af00f-bf35-4842-f62e-85121f752412"},"outputs":[],"source":["\"\"\"Loss checking\"\"\"\n","loss = []\n","model = copy.deepcopy(init_model) \n","path= f'./models/v127/2'\n","handler = ModelHandler(model, 1e6, path= f'./models/v2/', kfold=True)\n","batches = next(os.walk(path))[1]\n","loss_ = []\n","epochs_ = []\n","for batch in batches:\n","    batchpath = path+ '/' + batch\n","    handler.load(batchpath)\n","    loss_.append(handler.best_loss.cpu())\n","loss.append(loss_)\n","loss = np.array(loss)*2500\n","print(str(np.mean(loss))+ '±' + str(np.std(loss)))\n","loss"]},{"cell_type":"code","execution_count":null,"id":"Vu4jzm6aODXM","metadata":{"id":"Vu4jzm6aODXM"},"outputs":[],"source":["def load(path_):\n","    if path_.split('.')[-1] == 'pt':\n","        path = '/'.join(path.split('/')[:-1])\n","        name = path_.split('/')[-1]\n","        checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n","        loss = [checkpoint['loss'].numpy()]\n","    else:\n","        path = path_\n","        files = next(os.walk(path_))[2]\n","        x = lambda x: int(x.split('_')[-1].split('.')[0].split('(')[0])\n","        files.sort(key=x)\n","        loss = []\n","        for name in files:\n","            checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n","            loss.append(checkpoint['loss'].numpy())\n","    return np.array(loss)\n"]},{"cell_type":"code","execution_count":null,"id":"7ae63b29","metadata":{},"outputs":[],"source":["path = f'./models/v10'\n","loss = {}\n","for root, dirs, files in os.walk(path):\n","    if dirs:\n","        continue\n","    if not files:\n","        continue\n","\n","    loss[root] = load(root)*2500\n","loss"]},{"cell_type":"code","execution_count":null,"id":"afd95124","metadata":{},"outputs":[],"source":["def collate_batch_named(batch, named=True):\n","    sample_list = []\n","    label_list = []\n","    pouch = []\n","    cycle = []\n","    for i in batch:\n","        sample = np.stack([i['E'], i['I']],axis=-1)\n","        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n","        label_list.append(i['SoH'])\n","        pouch.append(i['Pouch'])\n","        cycle.append(i['Cycle'])\n","    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n","    labels = torch.tensor(label_list, dtype=torch.float32)\n","    if not named:\n","        return sequence_pad, labels    \n","    else:\n","        return sequence_pad, labels, pouch, cycle"]},{"cell_type":"code","execution_count":null,"id":"5c43337c","metadata":{},"outputs":[],"source":["def detailed_loss(handler, val_loader):\n","    with torch.no_grad():\n","        loss = []\n","        pouches = []\n","        soh = []\n","        pred = []\n","        cycles = []\n","        for data, labels, pouch, cycle in val_loader:\n","            preds = handler.best_model(data)\n","            pred.extend(preds)\n","            loss_ = criterion(preds, labels)\n","            loss.append(loss_)\n","            pouches.extend(pouch)\n","            cycles.extend(cycle)\n","            soh.extend(labels)\n","        \n","        loss = torch.hstack(loss)\n","        soh = torch.Tensor(soh)*50+50\n","        pred = torch.Tensor(pred)*50+50\n","        loss *= 2500 \n","        results = pd.DataFrame({'Pouch':pouches,'Cycle':cycles, 'Loss':loss, 'SoH':soh,'Pred':pred} )\n","        # results.sort_values(by='Loss',axis=0, ascending=0,inplace=True)\n","        return results"]},{"cell_type":"code","execution_count":null,"id":"593e1393","metadata":{},"outputs":[],"source":["batch_size = 64\n","model_version = 10\n","losses = []\n","for k in range(3):\n","    for s in sampling:\n","        datapath = f'data/v4/{s}/'\n","        modelpath = f'./models/v{model_version}/{s}/{k}'\n","        dataset_validate = GCPL_dataset_resampled3(datapath+'validate')\n","        dataset_rs2 = GCPL_dataset_resampled3(datapath+'main')\n","        main_loader = torch.utils.data.DataLoader(dataset_rs2, batch_size, shuffle=False, collate_fn=collate_batch_named)\n","        val_loader = torch.utils.data.DataLoader(dataset_validate, batch_size, shuffle=False, collate_fn=collate_batch_named)\n","        for i in next(os.walk(modelpath))[1]:\n","            init_model = copy.deepcopy(models[k])\n","            path = modelpath + f'/{i}'\n","            print(path)\n","            handler = ModelHandler(init_model)\n","            handler.load(path)\n","            result_main = detailed_loss(handler, main_loader)\n","            result_val = detailed_loss(handler, val_loader)\n","            result = pd.concat([result_main, result_val])\n","            loss = pd.concat([result.groupby('Pouch').Loss.mean(), result.groupby('Pouch').Loss.std()], axis=1)\n","            loss.columns = [f'{s}/{k}/{i}_Mean', f'{s}/{k}/{i}_Std']\n","            losses.append(loss)\n","f_loss = pd.concat(losses, axis=1)\n"]},{"cell_type":"code","execution_count":null,"id":"9dd8cbf3","metadata":{},"outputs":[],"source":["f_loss.iloc[:, 0::2]"]},{"cell_type":"code","execution_count":null,"id":"c46e8e2f","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"53b54a89","metadata":{},"outputs":[],"source":["losses"]},{"cell_type":"code","execution_count":null,"id":"863ad1c7","metadata":{},"outputs":[],"source":["losses"]},{"cell_type":"code","execution_count":null,"id":"7d1e95d7","metadata":{},"outputs":[],"source":["losses"]},{"cell_type":"code","execution_count":null,"id":"c58cc540","metadata":{},"outputs":[],"source":["losses__"]},{"cell_type":"code","execution_count":null,"id":"3d5245b5","metadata":{},"outputs":[],"source":["losses__"]},{"cell_type":"code","execution_count":null,"id":"pk6GHM8aauNh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59898,"status":"ok","timestamp":1682674442916,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"pk6GHM8aauNh","outputId":"09fe0165-2b74-400b-e78d-2eb7a4e4ed3b"},"outputs":[],"source":["loss = {}\n","for s in sampling:\n","    l_ = {}\n","    for i in range(3):\n","        path = f'./models/v10/{s}/{i}'\n","        batches = next(os.walk(path))[1]\n","        loss_ = []\n","        for batch in batches:\n","            batchpath = path+ '/' + batch\n","            a = load(batchpath)\n","            loss_.append(a)\n","        loss_ = torch.tensor(loss_)*2500\n","        l_[i] = loss_\n","        print(s, i, f'{loss_.mean():.3}±{loss_.std():.3}')\n","    loss[s] = l_"]},{"cell_type":"code","execution_count":null,"id":"3ykpy2gbcdkg","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":55409,"status":"ok","timestamp":1683888730433,"user":{"displayName":"Нейронные Сети","userId":"01091270931057423638"},"user_tz":-180},"id":"3ykpy2gbcdkg","outputId":"79a190cf-4f4c-4f20-f62b-ed33e731c118"},"outputs":[],"source":["length = []\n","soh = []\n","info = pd.DataFrame({\n","            'Pouch':[],\n","            'SoH':[],\n","            \"Len\":[],\n","            })\n","for i in range(len(dataset_rs2)):\n","    one = len(dataset_rs2[i]['E'])\n","    soh = dataset_rs2[i]['SoH']*50+50\n","    if np.any(np.isnan(dataset_rs2[i]['E'])):\n","        print(i)\n","    pouch = dataset_rs2[i]['Pouch']\n","    if i%1000 == 0:\n","        print(i)\n","    info.loc[len(info)] = [pouch, soh, one]\n","info"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"10T-4Lnr_W-hAUE0CUqiZH5X1e3TqH8EA","timestamp":1682610085757},{"file_id":"1VZGflUnrfvhb6fcO4lo1vyKcm0VTFMsX","timestamp":1681325357615}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}
