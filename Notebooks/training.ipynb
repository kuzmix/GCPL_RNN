{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"For google colab workflow - mounts Google Drive and go to it\"\"\"\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "os.chdir('./gdrive/MyDrive/Projects/RNN_for_GCPL/Notebooks')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d89eca-faca-4d2a-8308-5d23e961cb86",
   "metadata": {
    "executionInfo": {
     "elapsed": 8925,
     "status": "ok",
     "timestamp": 1683981601379,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "d7d89eca-faca-4d2a-8308-5d23e961cb86",
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-06-08T21:57:11.108071800Z",
     "start_time": "2023-06-08T21:56:39.354627200Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Imports all necessary libs\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn\n",
    "from Code.setup import *\n",
    "import datetime as dt\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from RNN_for_GCPL import setup\n",
    "\n",
    "\n",
    "seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae86c06f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"For compatibility - cd to folder with data and models\"\"\"\n",
    "os.chdir('../../RNN_for_GCPL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "123c730c",
   "metadata": {
    "executionInfo": {
     "elapsed": 4985,
     "status": "ok",
     "timestamp": 1683981757641,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "123c730c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Dataset consists of \"main\" an \"validate\" parts \"\"\"\n",
    "dataset_path = os.path.normpath(r'./data/v4/30/')\n",
    "\n",
    "dataset_main_path = os.path.join(dataset_path, 'main')\n",
    "dataset_val_path = os.path.join(dataset_path, 'validate')\n",
    "\n",
    "dataset_val = GCPL_dataset_resampled3(dataset_val_path)\n",
    "dataset_main = GCPL_dataset_resampled3(dataset_main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8df914",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683983697695,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "7b8df914",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 2\n",
    "output_dim = 20\n",
    "num_layers = 2\n",
    "bidir= True\n",
    "lr = 5e-4\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "init_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
    "best_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
    "\n",
    "gkf = GroupKFold(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QpCEkaxLX44A",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683981655651,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "QpCEkaxLX44A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\"\"\"number of neurons, number of layers, bidirectional\"\"\"\n",
    "params = [(20, 2, False),\n",
    "          (20, 2, True),\n",
    "          (30, 3, True)]\n",
    "for a,b,c in params:\n",
    "    model = MyGRU(input_dim, a, num_layers=b, bidir=c)\n",
    "    models.append(model)\n",
    "sampling = [30, 60, 120, 180, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_a7AUIWKTCdT",
   "metadata": {
    "id": "_a7AUIWKTCdT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probs = balancing(dataset_main, 5)\n",
    "soh, info = statistics(dataset_main)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db179b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BucketSampler(torch.utils.data.Sampler):\n",
    "    \n",
    "    def __init__(self, lengths, buckets=(50,500,50), shuffle=True, batch_size=32, drop_last=False):\n",
    "        \n",
    "        super().__init__(lengths)\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        assert isinstance(buckets, tuple)\n",
    "        bmin, bmax, bstep = buckets\n",
    "        assert (bmax - bmin) % bstep == 0\n",
    "        \n",
    "        buckets = defaultdict(list)\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length > bmin:\n",
    "                bucket_size = min((length // bstep) * bstep, bmax)\n",
    "                buckets[bucket_size].append(i)\n",
    "                \n",
    "        self.buckets = dict()\n",
    "        for bucket_size, bucket in buckets.items():\n",
    "            if len(bucket) > 0:\n",
    "                self.buckets[bucket_size] = torch.tensor(bucket, dtype=torch.int, device='cpu')\n",
    "        \n",
    "        # call __iter__() to store self.length\n",
    "        self.__iter__()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            for bucket_size in self.buckets.keys():\n",
    "                self.buckets[bucket_size] = self.buckets[bucket_size][torch.randperm(self.buckets[bucket_size].nelement())]\n",
    "                \n",
    "        batches = []\n",
    "        for bucket in self.buckets.values():\n",
    "            curr_bucket = torch.split(bucket, self.batch_size)\n",
    "            if len(curr_bucket) > 1 and self.drop_last == True:\n",
    "                if len(curr_bucket[-1]) < len(curr_bucket[-2]):\n",
    "                    curr_bucket = curr_bucket[:-1]\n",
    "            batches += curr_bucket\n",
    "            \n",
    "        self.length = len(batches)\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(batches)\n",
    "            \n",
    "        return iter(batches)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9d62a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch_length(batch):\n",
    "    sample_list = []\n",
    "    label_list = []\n",
    "    lengths = []\n",
    "    for i in batch:\n",
    "        sample = np.stack([i['E'], i['I']],axis=-1)\n",
    "        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n",
    "        label_list.append(i['SoH'])\n",
    "        lengths.append(len(i['E']))\n",
    "    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n",
    "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
    "    lengths_torch = torch.tensor(lengths, dtype=torch.float32)\n",
    "    return sequence_pad, labels, lengths_torch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a014d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probs = balancing(dataset_main, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c4970",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "missed_length = []\n",
    "n_bins = 10\n",
    "\n",
    "for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_main, groups=info.Pouch)):\n",
    "    seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
    "    prob_train = probs.copy()\n",
    "    prob_train[val_indices] = 0\n",
    "    balance = torch.multinomial(torch.tensor(prob_train) , len(train_indices), replacement=True)\n",
    "    train_set = torch.utils.data.Subset(dataset_main, balance)\n",
    "    \n",
    "    min_length =  info.loc[train_indices,'Len'].min()\n",
    "    max_length = info.loc[train_indices,'Len'].max()\n",
    "    max_length += n_bins - (max_length-min_length)% n_bins\n",
    "    print(min_length, max_length)\n",
    "    val_set = torch.utils.data.Subset(dataset_main, val_indices)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n",
    "    bucket_sampler = BucketSampler(info.loc[balance.numpy(),'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler, collate_fn=collate_batch_length)\n",
    "    # train_loader = torch.utils.data.DataLoader(train_set, batch_size =batch_size, shuffle=True, collate_fn=collate_batch_length)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    miss_ = []\n",
    "    for data, labels, length in train_loader:\n",
    "        m = data.shape[0]- length.mean()\n",
    "        miss_.append([m, data.shape[0],length.mean()])\n",
    "    miss_ = torch.tensor(miss_)\n",
    "    missed_length.append(miss_)\n",
    "    print(miss_.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96838dc7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_main, groups=info.Pouch)):\n",
    "    train_set = torch.utils.data.Subset(dataset_main, train_indices)\n",
    "    probs = balancing(train_set, 10)\n",
    "    prob_train = probs.copy()\n",
    "    prob_train[val_indices] = 0\n",
    "\n",
    "    balance = torch.multinomial(torch.tensor(prob_train) , len(train_indices), replacement=True)\n",
    "\n",
    "    plt.hist(info.loc[balance.numpy(), 'SoH'], 100, label=k)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb4c33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 1189374,
     "status": "ok",
     "timestamp": 1683993085823,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "a6bb4c33",
    "outputId": "eb71b146-75b8-40cd-ab1e-5e35dca8d0b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "draw_every = 25\n",
    "num_epochs = 75\n",
    "save_every = 10\n",
    "batch_size = 64\n",
    "n_bins = 10\n",
    "_ , ind = statistics(dataset_main)\n",
    "for m in sampling:\n",
    "    dataset_main = GCPL_dataset_resampled3(f'data/v4/{m}/main')\n",
    "    dataset_val = GCPL_dataset_resampled3(f'data/v4/{m}/validate')\n",
    "    for l, init_model in enumerate(models):\n",
    "        path = f'./models/v10/{m}/{l}'\n",
    "        for k, (train_indices, val_indices) in enumerate(gkf.split(dataset_main, groups=info.Pouch)):\n",
    "            seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
    "            min_length =  ind.loc[train_indices,'Len'].min()\n",
    "            max_length = ind.loc[train_indices,'Len'].max()\n",
    "            max_length += n_bins - (max_length-min_length)% n_bins\n",
    "            train_set = torch.utils.data.Subset(dataset_main, train_indices)\n",
    "            val_set = torch.utils.data.Subset(dataset_main, val_indices)\n",
    "            # sampler = torch.utils.data.WeightedRandomSampler(probs[train_indices], len(train_indices)) \n",
    "            # train_loader = torch.utils.data.DataLoader(train_set, batch_size, collate_fn=collate_batch, sampler=sampler)\n",
    "            # val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "            bucket_sampler_train = BucketSampler(ind.loc[train_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
    "            bucket_sampler_val = BucketSampler(ind.loc[val_indices,'Len'].to_numpy(), buckets = (min_length, max_length, n_bins), batch_size=batch_size)\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, batch_size =1,  batch_sampler=bucket_sampler_train, collate_fn=collate_batch)\n",
    "            model = copy.deepcopy(init_model)\n",
    "            val_loader = torch.utils.data.DataLoader(val_set, batch_size =1,  batch_sampler=bucket_sampler_val, collate_fn=collate_batch)\n",
    "            model = copy.deepcopy(init_model)\n",
    "            model.to(device)\n",
    "            optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            sheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "            handler = ModelHandler(model, 1e6, f'20 neurons, 2 layers, 120s sampling, 80% train, 20% val, k-fold No {k}.', path= path, kfold=True)\n",
    "            pp = ProgressPlotter()\n",
    "            counter = 0\n",
    "            for i in range(num_epochs):\n",
    "                model.train()\n",
    "                loss_val = 0\n",
    "                loss_train = 0\n",
    "                for data, labels in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(data.to(device))\n",
    "                    loss = torch.sum(criterion(preds, labels.to(device)))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_train += loss.detach().cpu()\n",
    "                \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    loss_train = loss_train/len(train_loader)\n",
    "                    pp.add_scalar('loss_train', loss_train.detach().cpu().numpy())\n",
    "                    model.eval()\n",
    "                    for data, labels in val_loader:\n",
    "                        preds = model(data.to(device))\n",
    "                        loss = torch.sum(criterion(preds, labels.to(device)))\n",
    "                        loss_val += loss.detach().cpu()\n",
    "                    loss_val = loss_val/len(val_set)\n",
    "                    sheduler.step(loss_val)\n",
    "                    handler.check_loss(loss_val, draw_every)\n",
    "                    pp.add_scalar('loss_val', loss_val.detach().cpu().numpy())\n",
    "                    if loss_val > handler.best_loss:\n",
    "                        counter +=1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "                    if (i+1)%draw_every == 0:\n",
    "                        pp.display([['loss_train', 'loss_val']])\n",
    "                    print(i, counter, loss_train*2500, loss_val*2500)\n",
    "                if (i+1)%save_every==0:\n",
    "                    handler.save(kfold_number=k)\n",
    "                    \n",
    "                    \n",
    "            handler.add_pp(pp)\n",
    "            handler.display()\n",
    "            handler.save(kfold_number=k)\n",
    "            with torch.no_grad():\n",
    "                loss_val = []\n",
    "                for data, labels in val_loader:\n",
    "                    preds = model(data.to(device))\n",
    "                    loss = criterion(preds, labels.to(device))\n",
    "                    loss_val.append(loss)\n",
    "                loss_val = torch.hstack(loss_val)\n",
    "                print(loss_val.mean(), loss_val.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5758cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xh_mcWwvFvr8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1682582736290,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "Xh_mcWwvFvr8",
    "outputId": "d50af00f-bf35-4842-f62e-85121f752412",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Loss checking\"\"\"\n",
    "loss = []\n",
    "model = copy.deepcopy(init_model) \n",
    "path= f'./models/v127/2'\n",
    "handler = ModelHandler(model, 1e6, path= f'./models/v2/', kfold=True)\n",
    "batches = next(os.walk(path))[1]\n",
    "loss_ = []\n",
    "epochs_ = []\n",
    "for batch in batches:\n",
    "    batchpath = path+ '/' + batch\n",
    "    handler.load(batchpath)\n",
    "    loss_.append(handler.best_loss.cpu())\n",
    "loss.append(loss_)\n",
    "loss = np.array(loss)*2500\n",
    "print(str(np.mean(loss))+ '±' + str(np.std(loss)))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vu4jzm6aODXM",
   "metadata": {
    "id": "Vu4jzm6aODXM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load(path_):\n",
    "    if path_.split('.')[-1] == 'pt':\n",
    "        path = '/'.join(path.split('/')[:-1])\n",
    "        name = path_.split('/')[-1]\n",
    "        checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n",
    "        loss = [checkpoint['loss'].numpy()]\n",
    "    else:\n",
    "        path = path_\n",
    "        files = next(os.walk(path_))[2]\n",
    "        x = lambda x: int(x.split('_')[-1].split('.')[0].split('(')[0])\n",
    "        files.sort(key=x)\n",
    "        loss = []\n",
    "        for name in files:\n",
    "            checkpoint = torch.load(path+ '/' + name, map_location=torch.device('cpu'))\n",
    "            loss.append(checkpoint['loss'].numpy())\n",
    "    return np.array(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae63b29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = f'./models/v10'\n",
    "loss = {}\n",
    "for root, dirs, files in os.walk(path):\n",
    "    if dirs:\n",
    "        continue\n",
    "    if not files:\n",
    "        continue\n",
    "\n",
    "    loss[root] = load(root)*2500\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd95124",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch_named(batch, named=True):\n",
    "    sample_list = []\n",
    "    label_list = []\n",
    "    pouch = []\n",
    "    cycle = []\n",
    "    for i in batch:\n",
    "        sample = np.stack([i['E'], i['I']],axis=-1)\n",
    "        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n",
    "        label_list.append(i['SoH'])\n",
    "        pouch.append(i['Pouch'])\n",
    "        cycle.append(i['Cycle'])\n",
    "    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n",
    "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
    "    if not named:\n",
    "        return sequence_pad, labels    \n",
    "    else:\n",
    "        return sequence_pad, labels, pouch, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43337c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detailed_loss(handler, val_loader):\n",
    "    with torch.no_grad():\n",
    "        loss = []\n",
    "        pouches = []\n",
    "        soh = []\n",
    "        pred = []\n",
    "        cycles = []\n",
    "        for data, labels, pouch, cycle in val_loader:\n",
    "            preds = handler.best_model(data)\n",
    "            pred.extend(preds)\n",
    "            loss_ = criterion(preds, labels)\n",
    "            loss.append(loss_)\n",
    "            pouches.extend(pouch)\n",
    "            cycles.extend(cycle)\n",
    "            soh.extend(labels)\n",
    "        \n",
    "        loss = torch.hstack(loss)\n",
    "        soh = torch.Tensor(soh)*50+50\n",
    "        pred = torch.Tensor(pred)*50+50\n",
    "        loss *= 2500 \n",
    "        results = pd.DataFrame({'Pouch':pouches,'Cycle':cycles, 'Loss':loss, 'SoH':soh,'Pred':pred} )\n",
    "        # results.sort_values(by='Loss',axis=0, ascending=0,inplace=True)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e1393",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model_version = 10\n",
    "losses = []\n",
    "for k in range(3):\n",
    "    for s in sampling:\n",
    "        datapath = f'data/v4/{s}/'\n",
    "        modelpath = f'./models/v{model_version}/{s}/{k}'\n",
    "        dataset_val = GCPL_dataset_resampled3(datapath + 'validate')\n",
    "        dataset_rs2 = GCPL_dataset_resampled3(datapath+'main')\n",
    "        main_loader = torch.utils.data.DataLoader(dataset_rs2, batch_size, shuffle=False, collate_fn=collate_batch_named)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset_val, batch_size, shuffle=False, collate_fn=collate_batch_named)\n",
    "        for i in next(os.walk(modelpath))[1]:\n",
    "            init_model = copy.deepcopy(models[k])\n",
    "            path = modelpath + f'/{i}'\n",
    "            print(path)\n",
    "            handler = ModelHandler(init_model)\n",
    "            handler.load(path)\n",
    "            result_main = detailed_loss(handler, main_loader)\n",
    "            result_val = detailed_loss(handler, val_loader)\n",
    "            result = pd.concat([result_main, result_val])\n",
    "            loss = pd.concat([result.groupby('Pouch').Loss.mean(), result.groupby('Pouch').Loss.std()], axis=1)\n",
    "            loss.columns = [f'{s}/{k}/{i}_Mean', f'{s}/{k}/{i}_Std']\n",
    "            losses.append(loss)\n",
    "f_loss = pd.concat(losses, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8cbf3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f_loss.iloc[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pk6GHM8aauNh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59898,
     "status": "ok",
     "timestamp": 1682674442916,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "pk6GHM8aauNh",
    "outputId": "09fe0165-2b74-400b-e78d-2eb7a4e4ed3b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = {}\n",
    "for s in sampling:\n",
    "    l_ = {}\n",
    "    for i in range(3):\n",
    "        path = f'./models/v10/{s}/{i}'\n",
    "        batches = next(os.walk(path))[1]\n",
    "        loss_ = []\n",
    "        for batch in batches:\n",
    "            batchpath = path+ '/' + batch\n",
    "            a = load(batchpath)\n",
    "            loss_.append(a)\n",
    "        loss_ = torch.tensor(loss_)*2500\n",
    "        l_[i] = loss_\n",
    "        print(s, i, f'{loss_.mean():.3}±{loss_.std():.3}')\n",
    "    loss[s] = l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ykpy2gbcdkg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 55409,
     "status": "ok",
     "timestamp": 1683888730433,
     "user": {
      "displayName": "Нейронные Сети",
      "userId": "01091270931057423638"
     },
     "user_tz": -180
    },
    "id": "3ykpy2gbcdkg",
    "outputId": "79a190cf-4f4c-4f20-f62b-ed33e731c118",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length = []\n",
    "soh = []\n",
    "info = pd.DataFrame({\n",
    "            'Pouch':[],\n",
    "            'SoH':[],\n",
    "            \"Len\":[],\n",
    "            })\n",
    "for i in range(len(dataset_rs2)):\n",
    "    one = len(dataset_rs2[i]['E'])\n",
    "    soh = dataset_rs2[i]['SoH']*50+50\n",
    "    if np.any(np.isnan(dataset_rs2[i]['E'])):\n",
    "        print(i)\n",
    "    pouch = dataset_rs2[i]['Pouch']\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    info.loc[len(info)] = [pouch, soh, one]\n",
    "info"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "10T-4Lnr_W-hAUE0CUqiZH5X1e3TqH8EA",
     "timestamp": 1682610085757
    },
    {
     "file_id": "1VZGflUnrfvhb6fcO4lo1vyKcm0VTFMsX",
     "timestamp": 1681325357615
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
