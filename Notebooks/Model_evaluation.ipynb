{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Imports all necessary libs\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn\n",
    "from Code.setup import *\n",
    "import datetime as dt\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from RNN_for_GCPL import setup\n",
    "\n",
    "\n",
    "seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"For compatibility - cd to folder with data and models\"\"\"\n",
    "os.chdir('../../RNN_for_GCPL/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Create new \"\"\"\n",
    "input_dim = 2\n",
    "output_dim = 20\n",
    "num_layers = 2\n",
    "bidir= True\n",
    "lr = 5e-4\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "init_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
    "best_model = MyGRU(input_dim, output_dim, num_layers=num_layers,bidir = bidir)\n",
    "\n",
    "gkf = GroupKFold(4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = []\n",
    "\"\"\"number of neurons, number of layers, bidirectional\"\"\"\n",
    "params = [(20, 2, False),\n",
    "          (20, 2, True),\n",
    "          (30, 3, True)]\n",
    "for a,b,c in params:\n",
    "    model = MyGRU(input_dim, a, num_layers=b, bidir=c)\n",
    "    models.append(model)\n",
    "sampling = [30, 60, 120, 180, 300]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# probs = balancing(dataset_main, 3)\n",
    "# soh, info = statistics(dataset_main+dataset_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BucketSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "    Bucket sampler from the internet.\n",
    "    \"\"\"\n",
    "    def __init__(self, lengths, buckets=(50,500,50), shuffle=True, batch_size=32, drop_last=False):\n",
    "\n",
    "        super().__init__(lengths)\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        assert isinstance(buckets, tuple)\n",
    "        bmin, bmax, bstep = buckets\n",
    "        assert (bmax - bmin) % bstep == 0\n",
    "\n",
    "        buckets = defaultdict(list)\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length > bmin:\n",
    "                bucket_size = min((length // bstep) * bstep, bmax)\n",
    "                buckets[bucket_size].append(i)\n",
    "\n",
    "        self.buckets = dict()\n",
    "        for bucket_size, bucket in buckets.items():\n",
    "            if len(bucket) > 0:\n",
    "                self.buckets[bucket_size] = torch.tensor(bucket, dtype=torch.int, device='cpu')\n",
    "\n",
    "        # call __iter__() to store self.length\n",
    "        self.__iter__()\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        if self.shuffle == True:\n",
    "            for bucket_size in self.buckets.keys():\n",
    "                self.buckets[bucket_size] = self.buckets[bucket_size][torch.randperm(self.buckets[bucket_size].nelement())]\n",
    "\n",
    "        batches = []\n",
    "        for bucket in self.buckets.values():\n",
    "            curr_bucket = torch.split(bucket, self.batch_size)\n",
    "            if len(curr_bucket) > 1 and self.drop_last == True:\n",
    "                if len(curr_bucket[-1]) < len(curr_bucket[-2]):\n",
    "                    curr_bucket = curr_bucket[:-1]\n",
    "            batches += curr_bucket\n",
    "\n",
    "        self.length = len(batches)\n",
    "\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(batches)\n",
    "\n",
    "        return iter(batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_batch_named(batch, named=True):\n",
    "    sample_list = []\n",
    "    label_list = []\n",
    "    pouch = []\n",
    "    cycle = []\n",
    "    filenames = []\n",
    "    for i in batch:\n",
    "        sample = np.stack([i['E'], i['I']],axis=-1)\n",
    "        sample_list.append(torch.tensor(sample, dtype=torch.float32))\n",
    "        label_list.append(i['SoH'])\n",
    "        pouch.append(i['Pouch'])\n",
    "        cycle.append(i['Cycle'])\n",
    "        filenames.append(i['Filename'])\n",
    "    sequence_pad = nn.utils.rnn.pad_sequence(sample_list)\n",
    "    labels = torch.tensor(label_list, dtype=torch.float32)\n",
    "    if not named:\n",
    "        return sequence_pad, labels\n",
    "    else:\n",
    "        return sequence_pad, labels, pouch, cycle, filenames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detailed_loss(handler, data_loader, criterion):\n",
    "    \"data_loader выдаёт 5 элемент - data, labels, pouch, cycle и filename\"\n",
    "    with torch.no_grad():\n",
    "        loss = []\n",
    "        pouches = []\n",
    "        soh = []\n",
    "        pred = []\n",
    "        cycles = []\n",
    "        filenames = []\n",
    "        for data, labels, pouch, cycle, filename in data_loader:\n",
    "            preds = handler.best_model(data)\n",
    "            pred.extend(preds)\n",
    "            loss_ = criterion(preds, labels)\n",
    "            loss.append(loss_)\n",
    "            pouches.extend(pouch)\n",
    "            cycles.extend(cycle)\n",
    "            soh.extend(labels)\n",
    "            filenames.extend(filename)\n",
    "\n",
    "        loss = torch.hstack(loss)\n",
    "        soh = torch.Tensor(soh)*50+50\n",
    "        pred = torch.Tensor(pred)*50+50\n",
    "        loss *= 2500\n",
    "        results = pd.DataFrame({'Pouch':pouches, 'Loss':loss, 'SoH':soh,'Pred':pred, 'Cycle':cycles, 'Filename':filenames} )\n",
    "        # results.sort_values(by='Loss',axis=0, ascending=0,inplace=True)\n",
    "        return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model_version = 15\n",
    "losses = []\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "dataset_version = 6\n",
    "s = 30\n",
    "k = 0\n",
    "n_bins = 30\n",
    "datapath = f'data/v{dataset_version}/{s}/'\n",
    "model_path = f'./models/v{model_version}/{s}/{k}'\n",
    "\n",
    "dataset_main = GCPL_dataset_resampled3(datapath)\n",
    "\n",
    "soh, info = statistics(dataset_main)\n",
    "\n",
    "min_length =  info['Len'].min() - 1\n",
    "max_length = info['Len'].max()\n",
    "bin_length = np.ceil((max_length - min_length)/n_bins)\n",
    "max_length += bin_length - (max_length-min_length)% bin_length\n",
    "bucket_sampler_main = BucketSampler(info['Len'].to_numpy(), buckets = (min_length, max_length, bin_length), batch_size=batch_size, shuffle=False)\n",
    "main_loader = torch.utils.data.DataLoader(dataset_main, batch_size =1,  batch_sampler=bucket_sampler_main, collate_fn=collate_batch_named)\n",
    "\n",
    "\n",
    "for i, (train_indices, val_indices) in enumerate(gkf.split(dataset_main, groups=info.Pouch)):\n",
    "    seedEverything(seed=DEFAULT_RANDOM_SEED)\n",
    "    path = model_path + f'/{i}'\n",
    "    print(path)\n",
    "    init_model = copy.deepcopy(models[k])\n",
    "    handler = ModelHandler(init_model)\n",
    "    handler.load(path)\n",
    "    result = detailed_loss(handler, main_loader, criterion)\n",
    "    result['Train/val'] = 'Train'\n",
    "    result.loc[val_indices, 'Train/val'] = 'Val'\n",
    "\n",
    "    losses.append(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = losses[2]\n",
    "loss[loss.Loss>30]['Loss'].hist(bins=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, (train_indices, val_indices) in enumerate(gkf.split(dataset_new, groups=info.Pouch)):"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
